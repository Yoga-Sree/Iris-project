# iris_classification.py
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import warnings
warnings.filterwarnings('ignore')

class IrisClassifier:
    def __init__(self):
        self.data = None
        self.X = None
        self.y = None
        self.X_train = None
        self.X_test = None
        self.y_train = None
        self.y_test = None
        self.scaler = StandardScaler()
        self.models = {}
        self.best_model = None
        
    def load_data(self):
        """Load and prepare the Iris dataset"""
        iris = load_iris()
        self.data = pd.DataFrame(iris.data, columns=iris.feature_names)
        self.data['target'] = iris.target
        self.data['species'] = self.data['target'].apply(lambda x: iris.target_names[x])
        
        # Separate features and target
        self.X = self.data.drop(['target', 'species'], axis=1)
        self.y = self.data['target']
        
        print("Dataset loaded successfully!")
        print(f"Dataset shape: {self.data.shape}")
        print("\nFirst 5 rows:")
        print(self.data.head())
        
    def explore_data(self):
        """Explore the dataset with visualizations"""
        print("\n=== Dataset Exploration ===\n")
        
        # Basic info
        print("Dataset Info:")
        print(self.data.info())
        
        # Statistical summary
        print("\nStatistical Summary:")
        print(self.data.describe())
        
        # Class distribution
        print("\nClass Distribution:")
        print(self.data['species'].value_counts())
        
        # Visualizations
        fig, axes = plt.subplots(2, 2, figsize=(12, 10))
        fig.suptitle('Iris Dataset Exploration', fontsize=16)
        
        # Feature distributions
        sns.histplot(data=self.data, x='sepal length (cm)', hue='species', kde=True, ax=axes[0, 0])
        sns.histplot(data=self.data, x='sepal width (cm)', hue='species', kde=True, ax=axes[0, 1])
        sns.histplot(data=self.data, x='petal length (cm)', hue='species', kde=True, ax=axes[1, 0])
        sns.histplot(data=self.data, x='petal width (cm)', hue='species', kde=True, ax=axes[1, 1])
        
        plt.tight_layout()
        plt.savefig('iris_distributions.png')
        plt.close()
        
        # Pairplot
        pairplot = sns.pairplot(self.data, hue='species', diag_kind='hist')
        pairplot.fig.suptitle('Pairplot of Iris Features', y=1.02)
        plt.savefig('iris_pairplot.png')
        plt.close()
        
        # Correlation heatmap
        plt.figure(figsize=(8, 6))
        sns.heatmap(self.data.drop('target', axis=1).corr(), annot=True, cmap='coolwarm', center=0)
        plt.title('Feature Correlation Heatmap')
        plt.tight_layout()
        plt.savefig('iris_correlation.png')
        plt.close()
        
        print("\nExploratory visualizations saved as PNG files.")
        
    def preprocess_data(self):
        """Split and scale the data"""
        # Split the data
        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(
            self.X, self.y, test_size=0.2, random_state=42, stratify=self.y
        )
        
        # Scale the features
        self.X_train = self.scaler.fit_transform(self.X_train)
        self.X_test = self.scaler.transform(self.X_test)
        
        print("\n=== Data Preprocessing ===\n")
        print(f"Training set shape: {self.X_train.shape}")
        print(f"Testing set shape: {self.X_test.shape}")
        
    def train_models(self):
        """Train multiple classification models"""
        print("\n=== Training Models ===\n")
        
        # Define models to train
        models = {
            'Logistic Regression': LogisticRegression(random_state=42),
            'K-Nearest Neighbors': KNeighborsClassifier(),
            'Support Vector Machine': SVC(random_state=42),
            'Decision Tree': DecisionTreeClassifier(random_state=42),
            'Random Forest': RandomForestClassifier(random_state=42)
        }
        
        # Train each model
        for name, model in models.items():
            model.fit(self.X_train, self.y_train)
            self.models[name] = model
            train_score = model.score(self.X_train, self.y_train)
            test_score = model.score(self.X_test, self.y_test)
            print(f"{name}: Train Accuracy = {train_score:.4f}, Test Accuracy = {test_score:.4f}")
            
        # Select the best model based on test accuracy
        self.best_model = max(self.models.items(), key=lambda x: x[1].score(self.X_test, self.y_test))
        print(f"\nBest Model: {self.best_model[0]} with test accuracy: {self.best_model[1].score(self.X_test, self.y_test):.4f}")
        
    def evaluate_model(self, model_name=None):
        """Evaluate the specified model or the best model"""
        if model_name is None:
            model = self.best_model[1]
            model_name = self.best_model[0]
        else:
            model = self.models[model_name]
            
        print(f"\n=== Evaluating {model_name} ===\n")
        
        # Make predictions
        y_pred = model.predict(self.X_test)
        
        # Calculate accuracy
        accuracy = accuracy_score(self.y_test, y_pred)
        print(f"Accuracy: {accuracy:.4f}")
        
        # Classification report
        print("\nClassification Report:")
        print(classification_report(self.y_test, y_pred, 
                                   target_names=load_iris().target_names))
        
        # Confusion matrix
        plt.figure(figsize=(8, 6))
        cm = confusion_matrix(self.y_test, y_pred)
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                    xticklabels=load_iris().target_names,
                    yticklabels=load_iris().target_names)
        plt.title(f'Confusion Matrix - {model_name}')
        plt.ylabel('True Label')
        plt.xlabel('Predicted Label')
        plt.tight_layout()
        plt.savefig(f'confusion_matrix_{model_name.lower().replace(" ", "_")}.png')
        plt.close()
        print(f"Confusion matrix saved as 'confusion_matrix_{model_name.lower().replace(' ', '_')}.png'")
        
    def predict_new_sample(self, sepal_length, sepal_width, petal_length, petal_width):
        """Predict the class of a new sample"""
        if self.best_model is None:
            print("Please train the model first!")
            return
            
        # Create sample array
        sample = np.array([[sepal_length, sepal_width, petal_length, petal_width]])
        
        # Scale the sample
        sample_scaled = self.scaler.transform(sample)
        
        # Make prediction
        prediction = self.best_model[1].predict(sample_scaled)
        probabilities = self.best_model[1].predict_proba(sample_scaled)
        
        # Get class name
        class_name = load_iris().target_names[prediction[0]]
        
        print(f"\n=== Prediction Result ===\n")
        print(f"Input Features: Sepal Length={sepal_length}, Sepal Width={sepal_width}, "
              f"Petal Length={petal_length}, Petal Width={petal_width}")
        print(f"Predicted Class: {class_name} (Class ID: {prediction[0]})")
        print("Prediction Probabilities:")
        for i, prob in enumerate(probabilities[0]):
            print(f"  {load_iris().target_names[i]}: {prob:.4f}")
            
        return class_name, probabilities

def main():
    """Main function to run the complete ML pipeline"""
    print("=" * 50)
    print("Iris Flower Classification Project")
    print("=" * 50)
    
    # Initialize the classifier
    iris_clf = IrisClassifier()
    
    # Load data
    iris_clf.load_data()
    
    # Explore data
    iris_clf.explore_data()
    
    # Preprocess data
    iris_clf.preprocess_data()
    
    # Train models
    iris_clf.train_models()
    
    # Evaluate the best model
    iris_clf.evaluate_model()
    
    # Make some sample predictions
    print("\n=== Sample Predictions ===")
    samples = [
        [5.1, 3.5, 1.4, 0.2],  # Setosa
        [6.0, 2.7, 5.1, 1.6],  # Virginica
        [5.7, 2.8, 4.1, 1.3]   # Versicolor
    ]
    
    for i, sample in enumerate(samples):
        print(f"\nSample {i+1}:")
        iris_clf.predict_new_sample(*sample)
    
    print("\n" + "=" * 50)
    print("Project completed successfully!")
    print("=" * 50)

if __name__ == "__main__":
    main()
